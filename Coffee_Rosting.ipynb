{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5763f8db-d9bb-408b-81ac-1aa688f33414",
   "metadata": {},
   "source": [
    "## Neural Network Model \n",
    "\n",
    "This neaural network model will predict weither the coffee is rosted good or bad using two perimeters `temperature` and `duration`. we will use tensor flow to achive the objective "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19a77966-e113-4190-bace-7d61d01a235b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb8cdd7-ebbe-46cc-91a7-f48f1b2df4c4",
   "metadata": {},
   "source": [
    "We willl use tensor flow to create a neural network model in tensorflow for coffee rosting dataset. First we will create a dataset using the function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b3a3f5a-47dc-4c53-9ee3-8ccb0a0cb035",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_coffee_data():\n",
    "    \"\"\" Creates a coffee roasting data set.\n",
    "        roasting duration: 12-15 minutes is best\n",
    "        temperature range: 175-260C is best\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(2)\n",
    "    X = rng.random(400).reshape(-1,2)\n",
    "    X[:,1] = X[:,1] * 4 + 11.5          # 12-15 min is best\n",
    "    X[:,0] = X[:,0] * (285-150) + 150  # 350-500 F (175-260 C) is best\n",
    "    Y = np.zeros(len(X))\n",
    "    \n",
    "    i=0\n",
    "    for t,d in X:\n",
    "        y = -3/(260-175)*t + 21\n",
    "        if (t > 175 and t < 260 and d > 12 and d < 15 and d<=y ):\n",
    "            Y[i] = 1\n",
    "        else:\n",
    "            Y[i] = 0\n",
    "        i += 1\n",
    "\n",
    "    return (X, Y.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be66c1a6-2629-4f3a-bbce-02d720a8db59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 2) (200, 1)\n"
     ]
    }
   ],
   "source": [
    "X,Y = load_coffee_data();\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5e402ef-9e05-4ea6-a4b2-255752e04bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Temperature, Minimum temperature :, 284.99, 151.32\n",
      "Maximum Duration, Minimum Duration :, 15.45, 11.51\n"
     ]
    }
   ],
   "source": [
    "print(f\"Maximum Temperature, Minimum temperature :, {np.max(X[:,0]):0.2f}, {np.min(X[:,0]):0.2f}\")\n",
    "print(f\"Maximum Duration, Minimum Duration :, {np.max(X[:,1]):0.2f}, {np.min(X[:,1]):0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0a02e0-3eef-4776-a3be-f244f9ec2cc0",
   "metadata": {},
   "source": [
    "#### Scaling the data\n",
    "We will scale the data of features before using it on the model. We have used normalization method to scale the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "67864dce-7616-4543-b91f-135ca0daae53",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_l=tf.keras.layers.Normalization(axis=-1)\n",
    "norm_l.adapt(X)\n",
    "Xn=norm_l(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "543bf960-1777-48db-92d6-7624d269a4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Temperature after normalization, Minimum temperature after Normalization :, 1.66, -1.69\n",
      "Maximum Duration after Normalization, Minimum Duration after Normalization :, 1.79, -1.70\n"
     ]
    }
   ],
   "source": [
    "print(f\"Maximum Temperature after normalization, Minimum temperature after Normalization :, {np.max(Xn[:,0]):0.2f}, {np.min(Xn[:,0]):0.2f}\")\n",
    "print(f\"Maximum Duration after Normalization, Minimum Duration after Normalization :, {np.max(Xn[:,1]):0.2f}, {np.min(Xn[:,1]):0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c898616a-d302-42e9-a8bb-7f58b41e74b0",
   "metadata": {},
   "source": [
    "`We will increase the size of the dataset to 200,000 rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a7f5ab1e-ceed-4ba0-a96c-6b2a7c133282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 2) (200000, 1)\n"
     ]
    }
   ],
   "source": [
    "Xt = np.tile(Xn,(1000,1))\n",
    "Yt= np.tile(Y,(1000,1))   \n",
    "print(Xt.shape, Yt.shape)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddde2810-cd98-42f7-99cb-9ef109bbbe83",
   "metadata": {},
   "source": [
    " In the above step we have increased the dataset from 2000 to 200,000 number of samples. Now we will instantiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c9f0b5ac-7d1e-42e6-8824-e684107ae3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234) #applied to achive consistent results\n",
    "model=Sequential(\n",
    "    [\n",
    "    tf.keras.Input(shape=(2,)),\n",
    "    Dense(3, activation='sigmoid', name='layer1'),\n",
    "    Dense(1, activation='sigmoid', name='layer2')              \n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "48a4bfa8-56db-45d5-9cb7-501dc3456f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " layer1 (Dense)              (None, 3)                 9         \n",
      "                                                                 \n",
      " layer2 (Dense)              (None, 1)                 4         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13\n",
      "Trainable params: 13\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fb0d6ed6-bad6-44c5-93c6-6c99a24403c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1, b1 = model.get_layer('layer1').get_weights()\n",
    "W2, b2 = model.get_layer('layer2').get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "706fb381-aa24-4e70-bcfa-709f0b56a33e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:\n",
      " [[ 0.55199647 -0.71500456  0.9824865 ]\n",
      " [-0.58328986 -0.5577629  -0.14954174]] \n",
      "b1: [0. 0. 0.]\n",
      "W2:\n",
      " [[1.1021022 ]\n",
      " [0.24180949]\n",
      " [0.7425983 ]] \n",
      "b2: [0.]\n"
     ]
    }
   ],
   "source": [
    "print(\"W1:\\n\", W1, \"\\nb1:\", b1)\n",
    "print(\"W2:\\n\", W2, \"\\nb2:\", b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec79eab9-06a0-40a7-a4bc-449ccd8428e2",
   "metadata": {},
   "source": [
    "Now we will compile the Model and fit it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5bf8f5f2-45ad-4588-ac18-976ce75416ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6250/6250 [==============================] - 6s 914us/step - loss: 0.2200\n",
      "Epoch 2/10\n",
      "6250/6250 [==============================] - 6s 916us/step - loss: 0.1450\n",
      "Epoch 3/10\n",
      "6250/6250 [==============================] - 6s 898us/step - loss: 0.1386\n",
      "Epoch 4/10\n",
      "6250/6250 [==============================] - 6s 905us/step - loss: 0.1360\n",
      "Epoch 5/10\n",
      "6250/6250 [==============================] - 6s 927us/step - loss: 0.1342\n",
      "Epoch 6/10\n",
      "6250/6250 [==============================] - 6s 928us/step - loss: 0.1331\n",
      "Epoch 7/10\n",
      "6250/6250 [==============================] - 6s 926us/step - loss: 0.1324\n",
      "Epoch 8/10\n",
      "6250/6250 [==============================] - 6s 975us/step - loss: 0.1317\n",
      "Epoch 9/10\n",
      "6250/6250 [==============================] - 6s 936us/step - loss: 0.1313\n",
      "Epoch 10/10\n",
      "6250/6250 [==============================] - 6s 955us/step - loss: 0.1309\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x159980bd0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(),\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    Xt,Yt,            \n",
    "    epochs=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70e692b-e65e-4732-b8b4-8dc708a282a1",
   "metadata": {},
   "source": [
    "### Updated weights for the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aae9b11e-6fb2-42bd-861a-085bad90ffbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:\n",
      " [[  5.8449574  10.809995    8.806063 ]\n",
      " [-15.924869    7.825997    2.5995893]] \n",
      "b1: [18.545805  -1.4267157 12.128967 ]\n",
      "W2:\n",
      " [[  7.0576506]\n",
      " [-22.796059 ]\n",
      " [  9.7351885]] \n",
      "b2: [-14.715465]\n"
     ]
    }
   ],
   "source": [
    "W1, b1 = model.get_layer(\"layer1\").get_weights()\n",
    "W2, b2 = model.get_layer(\"layer2\").get_weights()\n",
    "print(\"W1:\\n\", W1, \"\\nb1:\", b1)\n",
    "print(\"W2:\\n\", W2, \"\\nb2:\", b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a80fed-b25c-4565-b4f5-f72f011e4fa9",
   "metadata": {},
   "source": [
    "You can see that the values are different from what you printed before calling model.fit(). With these, the model should be able to discern what is a good or bad coffee roast.\n",
    "For the purpose of the next discussion, instead of using the weights you got right away, you will first set some weights we saved from a previous training run. This is so that this notebook remains robust to changes in Tensorflow over time. Different training runs can produce somewhat different results and the following discussion applies when the model has the weights you will load below.\n",
    "Feel free to re-run the notebook later with the cell below commented out to see if there is any difference. If you got a low loss after the training above (e.g. 0.002), then you will most likely get the same results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21b3f13-9220-4c18-b342-adcdbe142a50",
   "metadata": {},
   "source": [
    "### Predictions for the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e2c470-bc5a-428e-b008-1048a1b2f16b",
   "metadata": {},
   "source": [
    "We will make predicions on the test data which has to go through a simlr process of normalization before testing it with the model. here we are predictiing on the test data as given below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c45433c9-bb11-4044-8025-95e3897e501e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 202ms/step\n",
      "predictions = \n",
      " [[7.6872814e-01]\n",
      " [8.6474935e-13]]\n"
     ]
    }
   ],
   "source": [
    "X_test = np.array([\n",
    "    [200,13.9],  # positive example\n",
    "    [200,17]])   # negative example\n",
    "X_testn = norm_l(X_test)\n",
    "predictions = model.predict(X_testn)\n",
    "print(\"predictions = \\n\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ae0afc04-a65c-49e4-a9eb-ad3e31b5ab55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decisions = \n",
      "[[1.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "yhat = np.zeros_like(predictions)\n",
    "for i in range(len(predictions)):\n",
    "    if predictions[i] >= 0.5:\n",
    "        yhat[i] = 1\n",
    "    else:\n",
    "        yhat[i] = 0\n",
    "print(f\"decisions = \\n{yhat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb0033c-d88b-4af1-aa96-b168ddc8a575",
   "metadata": {},
   "source": [
    "The model predicted the first row as positive and the second row is negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff4c5e6-ed64-4832-a5db-da7056f73084",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
